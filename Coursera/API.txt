An API lets two pieces of software talk to each other For example you have your program, you have some data, you have other software components. You use the api to communicate with the api via inputs and outputs. Just like a function, you don’t have to know how the API works, but just its inputs and outputs. Pandas is actually a set of software components, much of which are not even written in Python. You have some data. You have a set of software components. We use the pandas api to process the data by communicating with the other Software Components. Let’s clean up the diagram. When you create a dictionary, and then create a pandas object with the Dataframe constructor, in API lingo, this is an “instance.” The data in the dictionary is passed along to the pandas API. You then use the dataframe to communicate with the API. When you call the method head, the dataframe communicates with the API displaying the first few rows of the dataframe. When you call the method mean the API will calculate the mean and return the values.
Play video starting at :1:32 and follow transcript1:32
REST APIs are another popular type of API; they allow you to communicate through the internet allowing you to take advantage of resources like storage, access more data, artificial intelligent algorithms, and much more. The RE stands for Representational, the S stands for State, the T stand for Transfer. In rest API’s your program is called the client. The API communicates with a web service you call through the internet. There is a set of rules regarding Communication, Input or Request, and Output or Response. Here are some common terms. You or your code can be thought of as a client. The web service is referred to as a resource. The client finds the service via an endpoint. We will review this more in the next section. The client sends requests to the resource and the the resource (web service) sends a response to the client. HTTP methods are a way of transmitting data over the internet We tell the Rest API’s what to do by sending a request. The request is usually communicated via an HTTP message. The HTTP message usually contains a JSON file. This contains instructions for what operation we would like the service to perform. This operation is transmitted to the webservice via the internet. The service performs the operation. In the similar manner, the webservice returns a response via an HTTP message, where the information is usually returned via a JSON file. This information is transmitted back to the client. Crypto Currency data is excellent to be used in an API because it is being constantly updated and it is vital to CryptoCurrency Trading We will use the Py-Coin-Gecko Python Client/Wrapper for the Coin Gecko API, updated every minute by Coin-Gecko We use the Wrapper/Client because it is easy to use so you can focus on the task of collecting data, we will also introduce pandas time series functions for dealing with time series data Using Py-Coin-Gecko to collect data is quite simple All we need is to install and import the library Create a client object And finally use a function to request our data. In this function we are getting data on bitcoin, in U.S. Dollars, for the past 30 days. In this case our response is a JSON expressed as a python dictionary of nested lists including price, market cap, and total volumes which contain the unix timestamp and the price at that time. We are only interested in price so that is what we will select using the key price To make things simple, we can convert our nested list to a DataFrame, with the columns time stamp and price its difficult to understand the column time stamp we Will convert it to a more readable format using the pandas Function to_datetime
Play video starting at :4:28 and follow transcript4:28
Using the to datetime function, we create readable time data, the input is the time stamp column unit of time is set to milliseconds We append the output to the new column date
Play video starting at :4:43 and follow transcript4:43
We. Would like to create a candle stick plot To get the data for the daily candlesticks we will group by the date to find the minimum, maximum, first, and last price of each day Finally we will use plotly to create the candlestick chart and plot it Now we can view the candlestick chart by opening the html file and clicking trust HTML in the top left of the tab It should look something like this

Introduction to web scraping
Web scraping, also known as web harvesting or web data extraction, is the process of extracting information from websites or web pages. It involves automated retrieval of data from web sources. People use it for various applications such as data analysis, mining, price comparison, content aggregation, and more.

How web scraping works
HTTP request
The process typically begins with an HTTP request. A web scraper sends an HTTP request to a specific URL, similar to how a web browser would when you visit a website. The request is usually an HTTP GET request, which retrieves the web page's content.

Web page retrieval
The web server hosting the website responds to the request by returning the requested web page's HTML content. This content includes the visible text and media elements and the underlying HTML structure that defines the page's layout.

HTML parsing
Once the HTML content is received, you need to parse the content. Parsing involves breaking down the HTML structure into components, such as tags, attributes, and text content. You can use BeautifulSoup in Python. It creates a structured representation of the HTML content that can be easily navigated and manipulated.

Data extraction
With the HTML content parsed, web scrapers can now identify and extract the specific data they need. This data can include text, links, images, tables, product prices, news articles, and more. Scrapers locate the data by searching for relevant HTML tags, attributes, and patterns in the HTML structure.

Data transformation
Extracted data may need further processing and transformation. For instance, you can remove HTML tags from text, convert data formats, or clean up messy data. This step ensures the data is ready for analysis or other use cases.

Storage
After extraction and transformation, you can store the scraped data in various formats, such as databases, spreadsheets, JSON, or CSV files. The choice of storage format depends on the specific project's requirements.

Automation
In many cases, scripts or programs automate web scraping. These automation tools allow recurring data extraction from multiple web pages or websites. Automated scraping is especially useful for collecting data from dynamic websites that regularly update their content.

Document Tree
HTML structure
Hypertext markup language (HTML) serves as the foundation of web pages. Understanding its structure is crucial for web scraping.

<html> is the root element of an HTML page.
<head> contains meta-information about the HTML page.
<body> displays the content on the web page, often the data of interest.
<h3> tags are type 3 headings, making text larger and bold, typically used for player names.
<p> tags represent paragraphs and contain player salary information.
Composition of an HTML tag
HTML tags define the structure of web content and can contain attributes.

An HTML tag consists of an opening (start) tag and a closing (end) tag.
Tags have names (<a> for an anchor tag).
Tags may contain attributes with an attribute name and value, providing additional information to the tag.
HTML document tree
You can visualize HTML documents as trees with tags as nodes.

Tags can contain strings and other tags, making them the tag's children.
Tags within the same parent tag are considered siblings.
For example, the <html> tag contains both <head> and <body> tags, making them descendants of <html but children of <html>. <head> and <body> are siblings.
Document Tree
HTML tables
HTML tables are essential for presenting structured data.

Define an HTML table using the <table> tag.
Each table row is defined with a <tr> tag.
The first row often uses the table header tag, typically <th>.
The table cell is represented by <td> tags, defining individual cells in a row.
HTML Table
Web scraping
Web scraping involves extracting information from web pages using Python. It can save time and automate data collection.

Required tools
Web scraping requires Python code and two essential modules: Requests and Beautiful Soup. Ensure you have both modules installed in your Python environment.

1
2
# Import Beautiful Soup to parse the web page content
from bs4 import BeautifulSoup
Copied!
Fetching and parsing HTML
To start web scraping, you need to fetch the HTML content of a webpage and parse it using Beautiful Soup. Here's a step-by-step example:

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
import requests
from bs4 import BeautifulSoup
# Specify the URL of the webpage you want to scrape
url = 'https://en.wikipedia.org/wiki/IBM'
# Send an HTTP GET request to the webpage
response = requests.get(url)
# Store the HTML content in a variable
html_content = response.text
# Create a BeautifulSoup object to parse the HTML
soup = BeautifulSoup(html_content, 'html.parser')
# Display a snippet of the HTML content
print(html_content[:500])
Copied!
Navigating the HTML structure
BeautifulSoup represents HTML content as a tree-like structure, allowing for easy navigation. You can use methods like find_all to filter and extract specific HTML elements. For example, to find all anchor tags () and print their text:

1
2
3
4
5
6
# Find all <a> tags (anchor tags) in the HTML
links = soup.find_all('a')
# Iterate through the list of links and print their text
for the link in links:
    print(link.text)
Copied!
Custom data extraction
Web scraping allows you to navigate the HTML structure and extract specific information based on your requirements. This process may involve finding specific tags, attributes, or text content within the HTML document.

Using BeautifulSoup for HTML parsing
Beautiful Soup is a powerful tool for navigating and extracting specific web page parts. It allows you to find elements based on their tags, attributes, or text, making extracting the information you're interested in easier.

Using pandas read_html for table extraction
Pandas, a Python library, provides a function called read_html, which can automatically extract data from websites' tables and present it in a format suitable for analysis. It’s similar to taking a table from a webpage and importing it into a spreadsheet for further analysis.

